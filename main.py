import asyncio
import json
import logging
import os
import shutil
from contextlib import AsyncExitStack
from typing import Any, Dict, List, Optional

import httpx
from dotenv import load_dotenv
from openai import OpenAI  # OpenAI Python SDK
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# é…ç½®æ—¥å¿—è¾“å‡ºåˆ°æ–‡ä»¶ï¼Œè€Œä¸æ˜¯æ§åˆ¶å°
log_dir = "logs"
if not os.path.exists(log_dir):
    os.makedirs(log_dir)
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(os.path.join(log_dir, "mcp_client.log")),
        # ä¸å†ä½¿ç”¨ StreamHandlerï¼Œé¿å…æ—¥å¿—è¾“å‡ºåˆ°æ§åˆ¶å°
    ]
)


class Configuration:
    """
    é…ç½®åŠ è½½ç±»

    ç®¡ç† MCP å®¢æˆ·ç«¯çš„ç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶
    """

    def __init__(self) -> None:
        load_dotenv()
        # ä»ç¯å¢ƒå˜é‡ä¸­åŠ è½½ API key, base_url å’Œ model
        self.api_key = os.getenv("LLM_API_KEY")
        self.base_url = os.getenv("BASE_URL")
        self.model = os.getenv("MODEL")
        if not self.api_key:
            raise ValueError("âŒ æœªæ‰¾åˆ° LLM_API_KEYï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®")

    @staticmethod
    def load_config(file_path: str) -> Dict[str, Any]:
        """
        ä» JSON æ–‡ä»¶åŠ è½½æœåŠ¡å™¨é…ç½®
        
        Args:
            file_path: JSON é…ç½®æ–‡ä»¶è·¯å¾„
        
        Returns:
            åŒ…å«æœåŠ¡å™¨é…ç½®çš„å­—å…¸
        """
        with open(file_path, "r") as f:
            return json.load(f)


class Server:
    """
    MCP æœåŠ¡å™¨å°è£…ç±»

    ç®¡ç†å•ä¸ª MCP æœåŠ¡å™¨è¿æ¥å’Œå·¥å…·è°ƒç”¨
    """

    def __init__(self, name: str, config: Dict[str, Any]) -> None:
        self.name: str = name
        self.config: Dict[str, Any] = config
        self.session: Optional[ClientSession] = None
        self.exit_stack: AsyncExitStack = AsyncExitStack()
        self._cleanup_lock = asyncio.Lock()

    async def initialize(self) -> None:
        """åˆå§‹åŒ–ä¸ MCP æœåŠ¡å™¨çš„è¿æ¥"""
        # command å­—æ®µç›´æ¥ä»é…ç½®è·å–
        command = self.config["command"]
        if command is None:
            raise ValueError("command ä¸èƒ½ä¸ºç©º")

        server_params = StdioServerParameters(
            command=command,
            args=self.config["args"],
            env={**os.environ, **self.config["env"]} if self.config.get("env") else None,
        )
        try:
            stdio_transport = await self.exit_stack.enter_async_context(
                stdio_client(server_params)
            )
            read_stream, write_stream = stdio_transport
            session = await self.exit_stack.enter_async_context(
                ClientSession(read_stream, write_stream)
            )
            await session.initialize()
            self.session = session
        except Exception as e:
            logging.error(f"Error initializing server {self.name}: {e}")
            await self.cleanup()
            raise

    async def list_tools(self) -> List[Any]:
        """è·å–æœåŠ¡å™¨å¯ç”¨çš„å·¥å…·åˆ—è¡¨

        Returns:
            å·¥å…·åˆ—è¡¨
        """
        if not self.session:
            raise RuntimeError(f"Server {self.name} not initialized")
        tools_response = await self.session.list_tools()
        tools = []
        for item in tools_response:
            if isinstance(item, tuple) and item[0] == "tools":
                for tool in item[1]:
                    tools.append(Tool(tool.name, tool.description, tool.inputSchema))
        return tools

    async def execute_tool(
        self, tool_name: str, arguments: Dict[str, Any], retries: int = 2, delay: float = 1.0
    ) -> Any:
        """æ‰§è¡ŒæŒ‡å®šå·¥å…·ï¼Œå¹¶æ”¯æŒé‡è¯•æœºåˆ¶

        Args:
            tool_name: å·¥å…·åç§°
            arguments: å·¥å…·å‚æ•°
            retries: é‡è¯•æ¬¡æ•°
            delay: é‡è¯•é—´éš”ç§’æ•°

        Returns:
            å·¥å…·è°ƒç”¨ç»“æœ
        """
        if not self.session:
            raise RuntimeError(f"Server {self.name} not initialized")
        attempt = 0
        while attempt < retries:
            try:
                logging.info(f"Executing {tool_name} on server {self.name}...")
                result = await self.session.call_tool(tool_name, arguments)
                return result
            except Exception as e:
                attempt += 1
                logging.warning(
                    f"Error executing tool: {e}. Attempt {attempt} of {retries}."
                )
                if attempt < retries:
                    logging.info(f"Retrying in {delay} seconds...")
                    await asyncio.sleep(delay)
                else:
                    logging.error("Max retries reached. Failing.")

                    # æ³¨æ„ï¼šè¿™é‡Œè¿”å›çš„æ˜¯é”™è¯¯ä¿¡æ¯ï¼Œè€Œä¸æ˜¯æ¥ç€æŠ›å¼‚å¸¸
                    return f"Error executing tool: {e}"
                    # raise

    async def cleanup(self) -> None:
        """æ¸…ç†æœåŠ¡å™¨èµ„æº"""
        async with self._cleanup_lock:
            try:
                await self.exit_stack.aclose()
                self.session = None
            except Exception as e:
                logging.error(f"Error during cleanup of server {self.name}: {e}")


class Tool:
    """
    å·¥å…·å°è£…ç±»

    å°è£… MCP è¿”å›çš„å·¥å…·ä¿¡æ¯
    """

    def __init__(self, name: str, description: str, input_schema: Dict[str, Any]) -> None:
        self.name: str = name
        self.description: str = description
        self.input_schema: Dict[str, Any] = input_schema

    def format_for_llm(self) -> str:
        """ç”Ÿæˆç”¨äº LLM æç¤ºçš„å·¥å…·æè¿°"""
        args_desc = []
        if "properties" in self.input_schema:
            for param_name, param_info in self.input_schema["properties"].items():
                arg_desc = f"- {param_name}: {param_info.get('description', 'No description')}"
                if param_name in self.input_schema.get("required", []):
                    arg_desc += " (required)"
                args_desc.append(arg_desc)
        return f"""
Tool: {self.name}
Description: {self.description}
Arguments:
{chr(10).join(args_desc)}
"""


class LLMClient:
    """
    LLM å®¢æˆ·ç«¯å°è£…ç±»ï¼ˆä½¿ç”¨ OpenAI SDK)

    ä½¿ç”¨ OpenAI SDK ä¸å¤§æ¨¡å‹äº¤äº’
    """

    def __init__(self, api_key: str, base_url: Optional[str], model: str) -> None:
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model = model

    def get_response(self, messages: List[Dict[str, Any]], tools: Optional[List[Dict[str, Any]]] = None) -> Any:
        """
        å‘é€æ¶ˆæ¯ç»™å¤§æ¨¡å‹ API,æ”¯æŒä¼ å…¥å·¥å…·å‚æ•°(function calling æ ¼å¼ï¼‰
        """
        payload = {
            "model": self.model,
            "messages": messages,
            "tools": tools,
            "stream": True,  # å¯ç”¨æµå¼æ¨¡å¼
        }
        try:
            logging.debug(f"Sending payload to LLM: {payload}")

            # å¤„ç†æµå¼å“åº”
            stream_resp = self.client.chat.completions.create(**payload)
            
            # æ”¶é›†å®Œæ•´å“åº”
            collected_chunks = []
            collected_messages = []
            collected_reasoning = []  # æ–°å¢ï¼šæ”¶é›†reasoning_content
            has_printed_content = False  # è·Ÿè¸ªæ˜¯å¦å·²ç»æ‰“å°è¿‡å†…å®¹
            has_printed_reasoning = False  # è·Ÿè¸ªæ˜¯å¦å·²ç»æ‰“å°è¿‡reasoning_content
            
            # æ”¶é›†å·¥å…·è°ƒç”¨ä¿¡æ¯
            tool_calls_info = {}  # ç”¨äºå­˜å‚¨åˆå¹¶çš„å·¥å…·è°ƒç”¨ä¿¡æ¯
            
            # å¤„ç†æµå¼å“åº”
            for chunk in stream_resp:
                collected_chunks.append(chunk)  # ä¿å­˜æ‰€æœ‰å—
                # æ‰“å°æ¯ä¸ªå—çš„å†…å®¹ï¼Œä¾¿äºè°ƒè¯•
                logging.debug(f"Stream chunk: {chunk}")
                
                if hasattr(chunk.choices[0], 'delta'):
                    # å¤„ç†å¸¸è§„å†…å®¹
                    if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content:
                        collected_messages.append(chunk.choices[0].delta.content)
                        # å®æ—¶æ‰“å°æµå¼å†…å®¹ç‰‡æ®µ
                        print(chunk.choices[0].delta.content, end="", flush=True)
                        has_printed_content = True
                    
                    # å¤„ç†reasoning_contentï¼ˆå¦‚æœæœ‰ï¼‰
                    if hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content:
                        content = chunk.choices[0].delta.reasoning_content
                        collected_reasoning.append(content)
                        
                        # åªæœ‰å½“æ²¡æœ‰å¸¸è§„contentæ—¶æ‰æ‰“å°reasoning_content
                        if not has_printed_content:
                            # é¿å…é‡å¤æ‰“å°ç›¸åŒçš„å†…å®¹ï¼ˆå¦‚"å¥½çš„"ï¼‰
                            if not has_printed_reasoning or (has_printed_reasoning and content != "å¥½çš„"):
                                print(content, end="", flush=True)
                                has_printed_reasoning = True
                    
                    # æ”¶é›†å·¥å…·è°ƒç”¨ä¿¡æ¯
                    if hasattr(chunk.choices[0].delta, 'tool_calls') and chunk.choices[0].delta.tool_calls:
                        for tool_call in chunk.choices[0].delta.tool_calls:
                            if tool_call.index is not None:
                                idx = tool_call.index
                                if idx not in tool_calls_info:
                                    tool_calls_info[idx] = {
                                        "id": tool_call.id if tool_call.id else "",
                                        "type": tool_call.type if tool_call.type else "function",
                                        "function": {
                                            "name": tool_call.function.name if hasattr(tool_call.function, 'name') and tool_call.function.name else "",
                                            "arguments": tool_call.function.arguments if hasattr(tool_call.function, 'arguments') and tool_call.function.arguments else ""
                                        }
                                    }
                                else:
                                    # åˆå¹¶ID
                                    if tool_call.id:
                                        tool_calls_info[idx]["id"] += tool_call.id
                                    
                                    # åˆå¹¶ç±»å‹
                                    if tool_call.type:
                                        tool_calls_info[idx]["type"] = tool_call.type
                                    
                                    # åˆå¹¶å‡½æ•°åç§°å’Œå‚æ•°
                                    if hasattr(tool_call, 'function'):
                                        if hasattr(tool_call.function, 'name') and tool_call.function.name:
                                            tool_calls_info[idx]["function"]["name"] = tool_call.function.name
                                        
                                        if hasattr(tool_call.function, 'arguments') and tool_call.function.arguments:
                                            tool_calls_info[idx]["function"]["arguments"] += tool_call.function.arguments
            
            # è¿™é‡Œä¿®æ­£äº†ç¼©è¿›é—®é¢˜ï¼Œå°†ä¸‹é¢çš„ä»£ç ç§»å‡ºå¾ªç¯
            print()  # æ¢è¡Œ
            
            # ä½¿ç”¨æœ€åä¸€ä¸ªå—ä½œä¸ºå®Œæ•´å“åº”
            if collected_chunks:
                full_response = collected_chunks[-1]
                
                # ç¡®ä¿å“åº”å¯¹è±¡æœ‰æ­£ç¡®çš„ç»“æ„
                if hasattr(full_response.choices[0], 'delta'):
                    # å¦‚æœdeltaä¸­æ²¡æœ‰contentå±æ€§æˆ–contentä¸ºNoneï¼Œè®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²
                    if not hasattr(full_response.choices[0].delta, 'content') or full_response.choices[0].delta.content is None:
                        # å¦‚æœæœ‰æ”¶é›†åˆ°çš„æ¶ˆæ¯ï¼Œä½¿ç”¨å®ƒä»¬
                        if collected_messages:
                            # ä¸ºdeltaæ·»åŠ contentå±æ€§
                            full_response.choices[0].delta.content = "".join(collected_messages)
                        elif collected_reasoning:  # å¦‚æœæœ‰reasoning_contentä½†æ²¡æœ‰å¸¸è§„content
                            # ä½¿ç”¨reasoning_contentä½œä¸ºcontent
                            full_response.choices[0].delta.content = "".join(collected_reasoning)
                        else:
                            # å¦åˆ™è®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²
                            full_response.choices[0].delta.content = ""
                    
                    # å¦‚æœæ˜¯å·¥å…·è°ƒç”¨ï¼Œå°†åˆå¹¶åçš„å·¥å…·è°ƒç”¨ä¿¡æ¯æ·»åŠ åˆ°å“åº”ä¸­
                    if full_response.choices[0].finish_reason == "tool_calls" and tool_calls_info:
                        # åˆ›å»ºåˆå¹¶åçš„å·¥å…·è°ƒç”¨åˆ—è¡¨
                        merged_tool_calls = []
                        for idx in sorted(tool_calls_info.keys()):
                            merged_tool_calls.append(tool_calls_info[idx])
                        
                        # å°†åˆå¹¶åçš„å·¥å…·è°ƒç”¨ä¿¡æ¯æ·»åŠ åˆ°å“åº”ä¸­
                        full_response.choices[0].delta.tool_calls = merged_tool_calls
                    
                    return full_response
                else:
                    raise Exception("å“åº”æ ¼å¼ä¸æ­£ç¡®,ç¼ºå°‘deltaå±æ€§")
            else:
                raise Exception("æ²¡æœ‰æ”¶åˆ°ä»»ä½•å“åº”å—")
        except (Exception, httpx.HTTPError) as e:
            # æ•è· LLM è°ƒç”¨è¿‡ç¨‹ä¸­çš„å¼‚å¸¸
            logging.error(f"Error during LLM call: {e}")
            raise


class MultiServerMCPClient:
    """
    å¤šæœåŠ¡å™¨ MCP å®¢æˆ·ç«¯ç±»
    
    é›†æˆé…ç½®æ–‡ä»¶ã€å·¥å…·æ ¼å¼è½¬æ¢ä¸ OpenAI SDK è°ƒç”¨
    """
    def __init__(self) -> None:
        """
        ç®¡ç†å¤šä¸ª MCP æœåŠ¡å™¨ï¼Œå¹¶ä½¿ç”¨ OpenAI Function Calling é£æ ¼çš„æ¥å£è°ƒç”¨å¤§æ¨¡å‹
        """
        self.exit_stack = AsyncExitStack()
        config = Configuration()
        self.openai_api_key = config.api_key
        self.base_url = config.base_url
        self.model = config.model
        self.client = LLMClient(self.openai_api_key, self.base_url, self.model)
        # (server_name -> Server å¯¹è±¡)
        self.servers: Dict[str, Server] = {}
        # å„ä¸ª server çš„å·¥å…·åˆ—è¡¨
        self.tools_by_server: Dict[str, List[Any]] = {}
        self.all_tools: List[Dict[str, Any]] = []

    async def connect_to_servers(self, servers_config: Dict[str, Any]) -> None:
        """
        æ ¹æ®é…ç½®æ–‡ä»¶åŒæ—¶å¯åŠ¨å¤šä¸ªæœåŠ¡å™¨å¹¶è·å–å·¥å…·
        servers_config çš„æ ¼å¼ä¸ºï¼š
        {
          "mcpServers": {
              "sqlite": { "command": "uvx", "args": [ ... ] },
              "puppeteer": { "command": "npx", "args": [ ... ] },
              ...
          }
        }
        """
        mcp_servers = servers_config.get("mcpServers", {})
        for server_name, srv_config in mcp_servers.items():
            server = Server(server_name, srv_config)
            await server.initialize()
            self.servers[server_name] = server
            tools = await server.list_tools()
            self.tools_by_server[server_name] = tools

            for tool in tools:
                # ç»Ÿä¸€é‡å‘½åï¼šserverName_toolName
                function_name = f"{server_name}_{tool.name}"
                self.all_tools.append({
                    "type": "function",
                    "function": {
                        "name": function_name,
                        "description": tool.description,
                        "input_schema": tool.input_schema
                    }
                })

        # è½¬æ¢ä¸º OpenAI Function Calling æ‰€éœ€æ ¼å¼
        self.all_tools = await self.transform_json(self.all_tools)

        logging.info("\nâœ… å·²è¿æ¥åˆ°ä¸‹åˆ—æœåŠ¡å™¨:")
        for name in self.servers:
            srv_cfg = mcp_servers[name]
            logging.info(f"  - {name}: command={srv_cfg['command']}, args={srv_cfg['args']}")
        logging.info("\næ±‡æ€»çš„å·¥å…·:")
        for t in self.all_tools:
            logging.info(f"  - {t['function']['name']}")

    async def transform_json(self, json_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å°†å·¥å…·çš„ input_schema è½¬æ¢ä¸º OpenAI æ‰€éœ€çš„ parameters æ ¼å¼ï¼Œå¹¶åˆ é™¤å¤šä½™å­—æ®µ
        """
        result = []
        for item in json_data:
            if not isinstance(item, dict) or "type" not in item or "function" not in item:
                continue
            old_func = item["function"]
            if not isinstance(old_func, dict) or "name" not in old_func or "description" not in old_func:
                continue
            new_func = {
                "name": old_func["name"],
                "description": old_func["description"],
                "parameters": {}
            }
            if "input_schema" in old_func and isinstance(old_func["input_schema"], dict):
                old_schema = old_func["input_schema"]
                new_func["parameters"]["type"] = old_schema.get("type", "object")
                new_func["parameters"]["properties"] = old_schema.get("properties", {})
                new_func["parameters"]["required"] = old_schema.get("required", [])
            new_item = {
                "type": item["type"],
                "function": new_func
            }
            result.append(new_item)
        return result

    async def chat_base(self, messages: List[Dict[str, Any]], max_tool_calls: int = 5) -> Any:
        """
        ä½¿ç”¨ OpenAI æ¥å£è¿›è¡Œå¯¹è¯,å¹¶æ”¯æŒå¤šæ¬¡å·¥å…·è°ƒç”¨(Function Calling).
        å¦‚æœè¿”å› finish_reason ä¸º "tool_calls",åˆ™è¿›è¡Œå·¥å…·è°ƒç”¨åå†å‘èµ·è¯·æ±‚.
        
        Args:
            messages: å¯¹è¯å†å²æ¶ˆæ¯
            max_tool_calls: æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯
        """
        response = self.client.get_response(messages, tools=self.all_tools)
        # å¦‚æœæ¨¡å‹è¿”å›å·¥å…·è°ƒç”¨
        tool_call_count = 0
        if response.choices[0].finish_reason == "tool_calls":
            while tool_call_count < max_tool_calls:
                tool_call_count += 1
                logging.info(f"æ‰§è¡Œç¬¬ {tool_call_count} æ¬¡å·¥å…·è°ƒç”¨")
                
                # åˆ›å»ºåŒ…å«å·¥å…·è°ƒç”¨ç»“æœçš„æ–°æ¶ˆæ¯
                messages = await self.create_function_response_messages(messages, response)
                
                # å‘é€æ–°æ¶ˆæ¯ç»™æ¨¡å‹
                response = self.client.get_response(messages, tools=self.all_tools)
                
                # å¦‚æœæ¨¡å‹ä¸å†è¿”å›å·¥å…·è°ƒç”¨ï¼Œåˆ™é€€å‡ºå¾ªç¯
                if response.choices[0].finish_reason != "tool_calls":
                    break
            
            if tool_call_count >= max_tool_calls:
                logging.warning(f"è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•° {max_tool_calls}ï¼Œå¼ºåˆ¶é€€å‡ºå·¥å…·è°ƒç”¨å¾ªç¯")
        
        return response

    async def create_function_response_messages(self, messages: List[Dict[str, Any]], response: Any) -> List[Dict[str, Any]]:
        """
        å°†æ¨¡å‹è¿”å›çš„å·¥å…·è°ƒç”¨è§£ææ‰§è¡Œï¼Œå¹¶å°†ç»“æœè¿½åŠ åˆ°æ¶ˆæ¯é˜Ÿåˆ—ä¸­
        """
        # åˆå§‹åŒ– function_call_messages ä¸ºç©ºåˆ—è¡¨
        function_call_messages = []
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæµå¼å“åº”ï¼Œå¹¶è·å–å·¥å…·è°ƒç”¨ä¿¡æ¯
        if hasattr(response.choices[0], 'delta') and hasattr(response.choices[0].delta, 'tool_calls'):
            # æµå¼å“åº”å¤„ç†
            if response.choices[0].delta.tool_calls is not None:  # ç¡®ä¿ tool_calls ä¸ä¸º None
                function_call_messages = response.choices[0].delta.tool_calls
                logging.debug(f"ä»æµå¼å“åº”ä¸­è·å–åˆ°çš„å·¥å…·è°ƒç”¨: {function_call_messages}")
                
                # æ·»åŠ æ¨¡å‹æ¶ˆæ¯åˆ°å†å²
                model_message = {"role": "assistant", "tool_calls": []}
                for tool_call in function_call_messages:
                    try:
                        model_message["tool_calls"].append({
                            "id": tool_call["id"] if isinstance(tool_call, dict) else tool_call.id,
                            "type": "function",
                            "function": {
                                "name": tool_call["function"]["name"] if isinstance(tool_call, dict) else tool_call.function.name,
                                "arguments": tool_call["function"]["arguments"] if isinstance(tool_call, dict) else tool_call.function.arguments
                            }
                        })
                    except (AttributeError, KeyError) as e:
                        logging.warning(f"å¤„ç†å·¥å…·è°ƒç”¨æ—¶å‡ºé”™: {e}")
                        continue
                messages.append(model_message)
        elif hasattr(response.choices[0], 'message') and hasattr(response.choices[0].message, 'tool_calls'):
            # éæµå¼å“åº”å¤„ç†
            if response.choices[0].message.tool_calls is not None:  # ç¡®ä¿ tool_calls ä¸ä¸º None
                function_call_messages = response.choices[0].message.tool_calls
                logging.debug(f"ä»éæµå¼å“åº”ä¸­è·å–åˆ°çš„å·¥å…·è°ƒç”¨: {function_call_messages}")
                
                # æ·»åŠ æ¨¡å‹æ¶ˆæ¯åˆ°å†å²
                try:
                    model_message = response.choices[0].message.model_dump()
                    messages.append(model_message)
                except AttributeError as e:
                    logging.warning(f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
                    # å°è¯•æ‰‹åŠ¨æ„å»ºæ¶ˆæ¯
                    model_message = {"role": "assistant", "tool_calls": []}
                    for tool_call in function_call_messages:
                        try:
                            model_message["tool_calls"].append({
                                "id": tool_call.id,
                                "type": "function",
                                "function": {
                                    "name": tool_call.function.name,
                                    "arguments": tool_call.function.arguments
                                }
                            })
                        except AttributeError:
                            continue
                messages.append(model_message)
        
        # å¤„ç†æ¯ä¸ªå·¥å…·è°ƒç”¨
        for function_call_message in function_call_messages:
            try:
                # æ”¯æŒå­—å…¸å’Œå¯¹è±¡ä¸¤ç§å½¢å¼
                if isinstance(function_call_message, dict):
                    tool_name = function_call_message["function"]["name"]
                    tool_args_str = function_call_message["function"]["arguments"]
                    tool_call_id = function_call_message["id"]
                else:
                    tool_name = function_call_message.function.name
                    tool_args_str = function_call_message.function.arguments
                    tool_call_id = function_call_message.id
                
                logging.info(f"å‡†å¤‡è°ƒç”¨å·¥å…·: {tool_name}, å‚æ•°: {tool_args_str}")
                
                # è§£æå‚æ•°
                try:
                    tool_args = json.loads(tool_args_str)
                except json.JSONDecodeError as e:
                    logging.error(f"è§£æå·¥å…·å‚æ•°æ—¶å‡ºé”™: {e}, åŸå§‹å‚æ•°: {tool_args_str}")
                    # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                    fixed_args_str = tool_args_str.replace("'", "\"")
                    try:
                        tool_args = json.loads(fixed_args_str)
                        logging.info(f"æˆåŠŸä¿®å¤å¹¶è§£æå‚æ•°: {tool_args}")
                    except:
                        # å¦‚æœä»ç„¶å¤±è´¥ï¼Œä½¿ç”¨ç©ºå­—å…¸
                        tool_args = {}
                    
                # è°ƒç”¨ MCP å·¥å…·
                function_response = await self._call_mcp_tool(tool_name, tool_args)
                logging.info(f"å·¥å…·è°ƒç”¨ç»“æœ: {function_response}")
                
                messages.append({
                    "role": "tool",
                    "content": function_response,
                    "tool_call_id": tool_call_id,
                })
            except Exception as e:
                logging.error(f"å¤„ç†å·¥å…·è°ƒç”¨æ—¶å‡ºé”™: {e}")
                continue
        
        return messages

    async def process_query(self, user_query: str) -> str:
        """
        OpenAI Function Calling æµç¨‹ï¼š
         1. å‘é€ç”¨æˆ·æ¶ˆæ¯ + å·¥å…·ä¿¡æ¯
         2. è‹¥æ¨¡å‹è¿”å› finish_reason ä¸º "tool_calls"ï¼Œåˆ™è§£æå¹¶è°ƒç”¨ MCP å·¥å…·
         3. å°†å·¥å…·è°ƒç”¨ç»“æœè¿”å›ç»™æ¨¡å‹ï¼Œè·å¾—æœ€ç»ˆå›ç­”
        """
        messages = [{"role": "user", "content": user_query}]
        response = self.client.get_response(messages, tools=self.all_tools)
        content = response.choices[0]
        logging.info(content)
        if content.finish_reason == "tool_calls":
            # å¤„ç†å·¥å…·è°ƒç”¨ï¼Œæ³¨æ„åŒºåˆ†æµå¼å’Œéæµå¼å“åº”
            tool_call = content.delta.tool_calls[0] if hasattr(content, 'delta') else content.message.tool_calls[0]
            tool_name = tool_call.function.name
            tool_args = json.loads(tool_call.function.arguments)
            logging.info(f"\n[ è°ƒç”¨å·¥å…·: {tool_name}, å‚æ•°: {tool_args} ]\n")
            result = await self._call_mcp_tool(tool_name, tool_args)
            
            # æ·»åŠ æ¨¡å‹æ¶ˆæ¯åˆ°å†å²
            if hasattr(content, 'delta'):
                messages.append(content.delta.model_dump())
            else:
                messages.append(content.message.model_dump())
                
            messages.append({
                "role": "tool",
                "content": result,
                "tool_call_id": tool_call.id,
            })
            response = self.client.get_response(messages, tools=self.all_tools)
            return response.choices[0].delta.content if hasattr(response.choices[0], 'delta') else response.choices[0].message.content
        return content.delta.content if hasattr(content, 'delta') else content.message.content

    async def _call_mcp_tool(self, tool_full_name: str, tool_args: Dict[str, Any]) -> str:
        """
        æ ¹æ® "serverName_toolName" æ ¼å¼è°ƒç”¨ç›¸åº” MCP å·¥å…·
        """
        parts = tool_full_name.split("_", 1)
        if len(parts) != 2:
            return f"æ— æ•ˆçš„å·¥å…·åç§°: {tool_full_name}"
        server_name, tool_name = parts
        server = self.servers.get(server_name)
        if not server:
            return f"æ‰¾ä¸åˆ°æœåŠ¡å™¨: {server_name}"
        resp = await server.execute_tool(tool_name, tool_args)

        # å¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²,å³execute_toolè°ƒç”¨å¤±è´¥æ—¶ï¼Œåˆ™ç›´æ¥è¿”å›
        if isinstance(resp, str):
            return resp
        else:
            return resp.content if resp.content else "å·¥å…·æ‰§è¡Œæ— è¾“å‡º"

    async def chat_loop(self) -> None:
        """å¤šæœåŠ¡å™¨ MCP + OpenAI Function Calling å®¢æˆ·ç«¯ä¸»å¾ªç¯"""
        logging.info("\nğŸ¤– å¤šæœåŠ¡å™¨ MCP + Function Calling å®¢æˆ·ç«¯å·²å¯åŠ¨ï¼è¾“å…¥ 'quit' é€€å‡ºã€‚")
        print("\nğŸ¤– å¤šæœåŠ¡å™¨ MCP + Function Calling å®¢æˆ·ç«¯å·²å¯åŠ¨ï¼è¾“å…¥ 'quit' é€€å‡ºã€‚")
        messages: List[Dict[str, Any]] = []
        while True:
            query = input("\nä½ : ").strip()
            if query.lower() == "quit":
                break
            # æ£€æŸ¥ç”¨æˆ·è¾“å…¥æ˜¯å¦ä¸ºç©º
            if not query:
                continue  # å¦‚æœè¾“å…¥ä¸ºç©ºï¼Œè·³è¿‡æœ¬æ¬¡å¾ªç¯
            try:
                messages.append({"role": "user", "content": query})
                messages = messages[-20:]  # ä¿æŒæœ€æ–° 20 æ¡ä¸Šä¸‹æ–‡
                print("\nAI: ", end="", flush=True)  # æå‰æ‰“å°AIå‰ç¼€ï¼Œåç»­å†…å®¹ä¼šåœ¨get_responseä¸­æµå¼è¾“å‡º
                response = await self.chat_base(messages)
                
                # å¤„ç†æµå¼å“åº”çš„ç»“æœ
                content = ""
                if hasattr(response.choices[0], 'delta'):
                    # æµå¼å“åº”
                    if hasattr(response.choices[0].delta, 'content') and response.choices[0].delta.content:
                        content = response.choices[0].delta.content
                    # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
                    messages.append({"role": "assistant", "content": content})
                else:
                    # éæµå¼å“åº”
                    content = response.choices[0].message.content
                    messages.append(response.choices[0].message.model_dump())
                
            except Exception as e:
                print(f"\nâš ï¸  è°ƒç”¨è¿‡ç¨‹å‡ºé”™: {e}")
                logging.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯")

    async def cleanup(self) -> None:
        """å…³é—­æ‰€æœ‰èµ„æº"""
        await self.exit_stack.aclose()


async def main() -> None:
    """ä¸»å‡½æ•°"""

    logging.basicConfig(level=logging.INFO)
    logging.info("åˆå§‹åŒ–...")
    # ä»é…ç½®æ–‡ä»¶åŠ è½½æœåŠ¡å™¨é…ç½®
    config = Configuration()
    servers_config = config.load_config("servers_config.json")
    client = MultiServerMCPClient()
    try:
        await client.connect_to_servers(servers_config)
        await client.chat_loop()
    finally:
        try:
            await asyncio.sleep(0.1)
            await client.cleanup()
        except RuntimeError as e:
            # å¦‚æœæ˜¯å› ä¸ºé€€å‡º cancel scope å¯¼è‡´çš„å¼‚å¸¸ï¼Œå¯ä»¥é€‰æ‹©å¿½ç•¥
            if "Attempted to exit cancel scope" in str(e):
                logging.info("é€€å‡ºæ—¶æ£€æµ‹åˆ° cancel scope å¼‚å¸¸ï¼Œå·²å¿½ç•¥ã€‚")
            else:
                raise

if __name__ == "__main__":
    asyncio.run(main())